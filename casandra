2 popular distribution of Casandra:
1) Data Stack Enterprise DSE Casandra
2) Appache Casandra

Toad supports Casandra as UI client
Data Stack client

Transparent Data Encrytion TDE is available wtih Data Stack and not Apache Casandra 

COnsistency can be finetuned at every transaction level


read hinted hand out on cordinator node in cassandra
hint default pile up time to replicate is 3 hours. 
after configured time, new hint data is not accepted and not replicted
beyond hint period, we cna run node tool repair to make each node have latest copy of  data.


tombstone is soft delete for file marked for deletion
tombstone expirey time be default is 10 days.
gr_grace_seconds is property for tombstone expiry
after this period, casandra deletes the files.
once data is marked for delete, you cannot retrieve that data from application

if node is down from more than gr_grace_periods, dont run node tool repair but instead rebuild your node
because when node is back uo it is a possibility that old data on those nodes are replicated to other nodes.
this data is called zombie data.
Rebuild node will means fresh data is replicated on this node from other nodes.

Read repair is another way to achieve the eventual consistency.

casandra nevers read before write, it never throws constraint violation exception
it will overide values, if required.

In CAP tehorem, Casssandra lies in AP part as consistentcy thorugh provided and tunable but not guaranteeed across

If consistency ALL is selected, then availability becomes a challenge.
so we use one for low consistency or chorum to read/write from majority of replicas.
use chorum for better consistency both in read and write

CQL and Thrift are just data reprentations to client. Data structure is same for both i.e. column family.
Thrift is now depricated after version casandra 2.1.8
But it can represent raw cassandra data model, so you need this client to understand casabdra version.

Casandra CLI is client for Thrift... CQLsh is client for SQL.

partiionkey column is part of primary key that is responsbile for sharding of data.
Thats why 1 column is required to be defined as primary key.
so select * from emp is not recommended as it will have no primary key/partiioner key to find out nodes for reading/writing
and this statement will go to all nodes.
Primary Key  has Parition key + clustering column(s)
parition key cna be combination of more than pone columns
partition key is used for creating storage partition
Clustering column is used for clustering of data and sorting of data at storage/paritionlevel

primary key ( (partion key1, partition key 2,..), clustering key 1, clustering key 2,...)


At raw level, data is shown as hexadceimal value in thrift cli 

With primary key as one field, the partition will be based on unique primary key.
With primary key as two columns, one partition key and second clustering column, the parition will be made on unique partiion key.
Each partition can be stored only on one node and dstributed across nodes
Line with name but empty value  is new line for display of Thrift

unit of data is partition...and partition is located by partition key
adjust your data model as per queries so that minimum parition are queried
partition size is limited by memory of node memory size.
So Volume of data is a important consideration for casandera data modeling
Bucketing is generally used to control parition size with column(s) that change periodically or dummy bucket id as part of parition key.

Write more about counters.
counter cannot be primary and cannot be initialized by insert.


COllections:
List is high level impementation of array in casandra
Set with unique values, no insertion order
Map with key value pair

Dynamic columns were acrifixed with thrift going away and collections were introduced to provide dynamicity

Partitioner gives token gives ring position and each node has mapping of ring position wiht partition id.

Default timeout of casandra query is 5 seconds.

By default clustering column is in ascending order. 
Use WITH CLUSTERING ORDER BY (activity DESC) to make descending order sort storage

system.dateof(activity) to see date string from UUID

Create Type: to define data strcutre
when you use type as datatype in tbale creation, you make the data type marked with FROZEN.
FROZEN means whole data will be seleced or updated in full data type.

Complext data type: key is not surrounded  by quotes but value are surrounded by quotes.
In dictionary even key is surrounded by quotes.

Time to Live TTL: given in seconds  and provides the longevity of data storage
USING TTL 120;
THis is not normally used as when you want to purge the data, you archive it first.
At table creation you can say WITH default_time_to_live = 120

STATIC data type identfier means all column will have same value.
If insert provides new value for a column, all previuos values will be updated to new colummn value.
static remain same for a parition

Use ALLOW FILTERING for query to be run without parition id in select column with where clause of non parition key.
You can define index on column for such cases so that performance is good and you need not use ALLOW FILTERING
Create Index emp_name_index on emp_table(emp_name)
Index creates mapping of emp_name and parition key emp_id.
When you delte data, tombstones will be created in index mapping internal table
To avoid index table, you can use table per query which is a denomalization exercise 
But mutiple tables need to be updated for any change in data for insert/update/delete.
This creates complexity in appplication code.
SO idea of MATERIALIZED_VIEW  is used to bypas this restriction
Cassandra does not have regular view.
Materialzed view creates a different table for a query.
You can create materialized view and query on experimental view.
Materialzed view has restrictions are still called as in Beta.
Materilzed orimary key should contain base table primary key and atmost inlcude one non primary key column
Data modification is allowed only on base tables. 
But you can have one base table but mutiple materialzed view for it to suppport non primary parition key 
Updates in base table is streame to materialzed view in almost real time

Use textasBlob to store blob data fro text and use blobtotext in read query to convert blob data to readable text

Data is stored by keyspace and then table name on physical store

Try not to change durable_writes in keyspace creation.By default it is true.

By defining clustering column, I can put more and more rows
We can only insert values less than 64kb in clustering cloumn

Static table does not have clustering column, so one is to one mapping between paritions and rows
Dynamic table has clustering column so one is to many mapping between paritions and rows

adarsh0007@gmail.com

use virtual node for tokens and if num_tokens is 256, it means node uses vnode


When data arrives on node, it get appended to commit log(disk) and then on mem table(memory)
Data is written in SS dable in compressed format


When these strucutres are full to a configured limit, data is flushed to SS(String sorted) table and commit log is rotated
SS table is immutable
Every logical table has its own commit log, mm table and ss table

Every column is appended with timestamp, so mem table or mutiple ss tables containing same dataset with updates will be resolved by latest timestampe
mem table/ss table can contain mutiple partition key value

Periodically compaction process runs to combine mutiple SS table.
It retaing teh data with latest timestamp.
The merged SS table is deleted fro disk.
While compacting the data for tombstone items,  check for gc lifecycle is done,
If gc_lifecycle time is passed, then compaction will not move it into merged ss table.
gc lifecylce means data will be deleted after that time period on compaction and does not mean exact data is deleteds exactly at end of lifecylce time.
So a lazy delete and gc
Remember tomstone are marked at column level, so column will tombstone with expried gc is not moved to merged SS table


In Reading, data is read from mem table and all active SS tables.
Reading uses Bloom filter.
Bloom filter file is created with SS table 
1st question: whether parition exist in SS table ot not?
If yes, bloom filter can give you false positive for data but never give you false negative
You can configure bloom filter to ignore false postive at time fo creation of table.
COmpression offset map wll be used to read data from Mem tbale and SS table

For reading, parition index on disk is made containing parition number and offset in SS table
Parition index itslef can be huge, so solve that problem weh have parition summary table in memory.
Parition summary cotains parition range with index mapping normally for batch of 40 and is loaded in memory.
Key cache is another table in memory table (for each SS table) for latest accessed/written parition and direct offset.
if key cahce has data, then read does not ask Parition summary and parition index

Read sequence
1) Bloom filter will tell whether parition exists in this node
2) Read will then go to Key Cahe, if data is hit then read will got to SS table to read and return teh data
3) If data is miss, then read will go to parition summary, get the index range in parition index
4) Read goes to Paririon index, get offset
5) Read uses compression map to read and decompress data, update key cache for recently accessed data
6) return the data




