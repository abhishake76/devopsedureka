
Error while powering on: This host supports Intel VT-x, but Intel VT-x is disabled.

Intel VT-x might be disabled if it has been disabled in the BIOS/firmware settings or the host has not been power-cycled since changing this setting.

(1) Verify that the BIOS/firmware settings enable Intel VT-x and disable 'trusted execution.'

(2) Power-cycle the host if either of these BIOS/firmware settings have been changed.

(3) Power-cycle the host if you have not done so since installing VMware Player.

(4) Update the host's BIOS/firmware to the latest version.

This host does not support "Intel EPT" hardware assisted MMU virtualization.

Module 'CPUIDEarly' power on failed.

Failed to start the virtual machine.

Step to fix Error:
Restart laptop >>> BIOS Settings >> Enable Intel VT-x / AMD -v


3 case studies 5+ pages

what is a small file problem?
what issues will come with 1 million small files?
what ideal block side should be for HDFS?
how to specify more than one path for storage?

single master in hadoop1 so single point of faiulre. resolved in hadoop2 with multiple instances of master

master stores all the metada in memoery so should have good memory spec
and should be installed on reliable, good spec hardware

Mid size spec with medium ran and high disc (16 tb 2*8 mid size) 

default block size of data is 128 mb in hadoop 3.0. earlier it was 64 Mb.

in hdfs-site.xml on master, we can increase or decrease the replication number

in one datanode also, data is tored on mutiple discs to improve disk i/o performance
Logical volume manager(LVM) for disk decreases i/o performance as it considers all disks as one disk
jbod uses different mount points for each disk and increases the i/o performance

if datanode does not send heartbeats for 2 min(configurable), then namenode considers datanode as dead node.
and once dead node is considered, name node will create replicas of blocks on that node to other nodes
if the node comes up, once Load balancer, kickup, it will delete additional data.
any of the blocks which is not getting accessed is deleted.
ideally what should be replication factor


if 65 data comes for 64 mb block, 64 mb data will go in 1st block, and 2nd block will be just 1 mb to not waster space.
only the last block size can vary

blocks distribution on datanodes is random with few algorithm principles:
1) store data on nearest node from client
2) load on datanode. if slave is too busy, then less blocks will be stored on that machine
3) hadoop is rack aware.

FINISH the quizes on datdalfair nexttel LMS.
HOW to enable trash in hadoop?

Q.I installed hadoop sucessfully. during jps command datanode is not displaying
A: stop hadoop services >> delete hdata dir >> format NN bin/hdfs namenode -format >> start services again

to show all commands
$ hadoop fs

to create dir
$ hadoop fs -mkdir /data001

files cannot be edited in hdfs, but files can be appended with appentToFile command
$ hadoop fs -appendToFile temp /data004/hdfs-site.xml


For small files say size 1 MB, block size will be same as file size. The block size shown on GUI is cluster Block size configured.



Map Reduce:
1) Used when user request for some data from hdfs
2) Request will be divided into mutiple tasks with each task being served by mapper. Each mapper tries to serve data from local node.
3) HDFS block with key, value pair of byteoffset and value is input to mapper. Map function receives one key-value pair at a time.
4) Output from mapper is called intermediate output and again contain proecssed key value pairs. Mapper output is stored on local disk and not in hdfs.
5) User write the custom program for map
6) Mutiple map function will run on different nodes for each request
7) Intermediate output in form of key, value pair is send to Reducer(s) which can be run one of the nodes running map function.
8) The same key with mutiple values from different mappers is fed into one Reducer as Reducer aggregate values of same key from different mappers.
9) The reducer output is again key.value pair and this final output is stored in hdfs system
10) This final output is then send/streamed back to client
11) We can configure this output file to be retained or deleted.















