
Error while powering on: This host supports Intel VT-x, but Intel VT-x is disabled.

Intel VT-x might be disabled if it has been disabled in the BIOS/firmware settings or the host has not been power-cycled since changing this setting.

(1) Verify that the BIOS/firmware settings enable Intel VT-x and disable 'trusted execution.'

(2) Power-cycle the host if either of these BIOS/firmware settings have been changed.

(3) Power-cycle the host if you have not done so since installing VMware Player.

(4) Update the host's BIOS/firmware to the latest version.

This host does not support "Intel EPT" hardware assisted MMU virtualization.

Module 'CPUIDEarly' power on failed.

Failed to start the virtual machine.

Step to fix Error:
Restart laptop >>> BIOS Settings >> Enable Intel VT-x / AMD -v


3 case studies 5+ pages

what is a small file problem?
what issues will come with 1 million small files?
what ideal block side should be for HDFS?
how to specify more than one path for storage?

single master in hadoop1 so single point of faiulre. resolved in hadoop2 with multiple instances of master

master stores all the metada in memoery so should have good memory spec
and should be installed on reliable, good spec hardware

Mid size spec with medium ran and high disc (16 tb 2*8 mid size) 

default block size of data is 128 mb in hadoop 3.0. earlier it was 64 Mb.

in hdfs-site.xml on master, we can increase or decrease the replication number

in one datanode also, data is tored on mutiple discs to improve disk i/o performance
Logical volume manager(LVM) for disk decreases i/o performance as it considers all disks as one disk
jbod uses different mount points for each disk and increases the i/o performance

if datanode does not send heartbeats for 2 min(configurable), then namenode considers datanode as dead node.
and once dead node is considered, name node will create replicas of blocks on that node to other nodes
if the node comes up, once Load balancer, kickup, it will delete additional data.
any of the blocks which is not getting accessed is deleted.
ideally what should be replication factor


if 65 data comes for 64 mb block, 64 mb data will go in 1st block, and 2nd block will be just 1 mb to not waster space.
only the last block size can vary

blocks distribution on datanodes is random with few algorithm principles:
1) store data on nearest node from client
2) load on datanode. if slave is too busy, then less blocks will be stored on that machine
3) hadoop is rack aware.

FINISH the quizes on datdalfair nexttel LMS.
HOW to enable trash in hadoop?

Q.I installed hadoop sucessfully. during jps command datanode is not displaying
A: stop hadoop services >> delete hdata dir >> format NN bin/hdfs namenode -format >> start services again

to show all commands
$ hadoop fs

to create dir
$ hadoop fs -mkdir /data001

files cannot be edited in hdfs, but files can be appended with appentToFile command
$ hadoop fs -appendToFile temp /data004/hdfs-site.xml


For small files say size 1 MB, block size will be same as file size. The block size shown on GUI is cluster Block size configured.



Map Reduce:
1) Used when user request for some data from hdfs
2) HDFS data for Request will be divided into mutiple split with each split being served by  individual mapper. 
   Each mapper tries to serve data from local node. 
   In rare cases if one node is too busy the block from that node will move to other node with mapper.
3) HDFS block with key, value pair of byteoffset and value is input to mapper. Map function receives one key-value pair at a time.
4) Output from mapper is called intermediate output and again contain proecssed key value pairs. Mapper output is stored on local disk and not in hdfs.
5) User write the custom program for map
6) Mutiple map function will run on different nodes for each request
7) Intermediate output in form of key, value pair is send to Reducer(s) which can be run one of the nodes running map function.
   The mapper output key pair can be different from input pair
8) The same key with mutiple values from different mappers is fed into one Reducer as Reducer aggregate values of same key from different mappers.
   Key value pairs are sorted by key before reduce funtion kicks.
   Reduce funtion recives one key and complete collection of values for that key.
   Reducer does not have to necessarily follow node locality
9) The reducer output is again key.value pair and called final output is stored in hdfs system.
   Output from each reducer is one single output file.
   The reducer output key pair can be different from input pair.
   FInal output as any hdfs file will be replicated by default.
10) This final output is then send/streamed back to client
11) We can configure this output file to be retained or deleted.

How many mappers can rum simultanously?
Depends on 2 things
1) Limit on number of mappers allowed per node. 
   Right number of parallelism for maps is between 10 to 100 maps/node depending on the RAM size as mappers will run in JVM and CPU cores, 
   for every map you can roughly estimate 1.5 core. For 15 cores you can give 10 mapper.
2) Total size of inputs
Each block will need one map. So number of blocks in input data from hdfs will determine total number of mappers required

How many reducers can run simultaneously?
Number of reducers is way less than number of mappers.
The number of reducers os 0.95 or 1.75 mutiplied by config param mapred.tasktracker.reduce.tasks.maximum.
With 0.95, many redcuers can launch immediately and start reducing mutiple map outputs as maps finish jobs.
With 1.75, faster nodes will finish first round of reducer and launch second wave of reducers to reduce again.

Sometime there can be map only job qithout a reduce with a final output written directly on hdfs and will be replicated.
No need of shuffling and no need of data movement
Map only jobs are more efficient that map reduce jobs as there is no data movement which is a very costly job/
Map reduce application are limited by the bandwidth available on  the cluster
Hadoop allows the user to specify; a combiner function on map output.
Combiner is a mini reducer function which operates only on data generated by one machine saving on data movement.
Comibner acts afiltering or an aggregating step to lessen the number of intermediate keys that are being passed to the reducer.
Mostly the reducer class is also used as combinor for optimization.

Data locality:
Move computation closer to the data.
This minim;izes network congestion
hdfs provides interfaces to move application closer to nodes where data is located.


While executing a mapreduce job on hadoop, for virtual limit exception change in map-site.xml yarn entry to local.

For any class to be "value" for map and reduce class, it has to implement org.apache.hadoop.io.Writeable
Two methods for overwrite
1) write (DataOutput out)
2) readFields (DataInput in)

For any class to be "Key" for map and reduce class, it has to implement org.apache.hadoop.io.WriteableComparable
Three methods for overwrite
1) write (DataOutput out)
2) readFields (DataInput in)
3) compareTo(To)

Process of Map Reduce
1) InputFormat is a class(component )defines how input file is divided and read.
It selects the file from hdfs store and defines the input SPLIT for reading
2) Split is a unit of mapreduce data processing and is a logical representation of a block on hdfs
   Block is physical unit of hdfs.
   Hadoop undersand line boundaries. 
   Lines are not divided between two blocks.
   It terminates the block size to previuos line where size of data is small than block size.
   With xml, it stores as multi line record
   A split will take data from one block but might read partial data from other block(local/remote) to get complete record.
   Therefore Split is logical record boundary and block is physical data boundary
3) RecordReader (RR) is the class that read each split i.e. read data from hdfs.
4) RecordReader keeps giving one key-value pair to Map class till entire split is processed.
   For each split/block their is a mapper.
   map method will be called as many times as key-value pairs are there
   But every mapper class has one setup and cleanup method which will be called only once.
   You can access inputFileName in this setup method. And can do any initialization work in setup method.
   Close or freeup resources in cleanup method.
5) Map result key-value pairs from local disk are comibned by combiner and sent to Paritioner class. 
   Partitioner takes hashvalue of key and all paritioner send same hashvalue(range)/keys data to same reducer node
   The process of moving map output to reducer is known as shuffling
   Shuffling is physcial movement of map results to reducer node.
   As soon as first map task is finished, shuffling starts and data is sent to reducer
   Shuffling is limited by network badnwidth so 10 Gig connection is recommended for intra node connection
6) Sorting is done on key value pairs from paritnioner
7) One Sorted key with mutiple values are given to one Reducer
8) Output of reducer is written to hdfs via outputformat class
   Output format class defines the way how key value pairs will be written to output files.
   Output format decides the location/block where results will be written
9) RecordWriter is the actual class which writes teh reduced task output to persistent hdfs store
   It write one key-value pair at a time.

Note for MapReduce you can customize RecordReader, Mapper (setup, clean, map, combiner), Parition, Reducer(), XMLInoutFormat, XMLOUtputFormat class.

Use jdom in Map Reduce for aml reading as it has simplicity of dom and quick performance of SAX.

FInd best reader for json in MapReduce

org.apache.hadoop.filecache.DistributedCache to get set metadata to distributed cache on alll nodes.
org.apache.hadoop.filecache.DistributedCache.addFiletoClassPath method to add jars to classpath

write hadoop command to execute a jar file for map reduce.

work on MovieAnalysis Project and HR_Analysis Doc.

Impala is a MPP (Massive Parallel Processing) SQL query engine for processing huge volumes of data that is stored in Hadoop cluster. 
It is an open source software which is written in C++ and Java. 
It provides high performance and low latency compared to other SQL engines for Hadoop.

YARN:
ResourceManager daemon on master Node.
NodeManager daenon on slave nodes 
ApplicationMaster is triggered every time a new MapReduce job is triggered
Resource Manager will talk to Node Manager to trigger a application master in a container on slave node.
Resource manager gives containers to application to run. Container is like a memory space and cpu allocatio like 1G, 1 CPU.
Application Master will request RM for one or more containers to run applications which RM will coordinate with NM to allocate more containers.

Kerberos is a computer network security protocol that authenticates service requests between two or more trusted hosts across an untrusted network
Kerberos authentication is currently the default authorization technology used by Microsoft Windows, 
and implementations of Kerberos exist in Apple OS, FreeBSD, UNIX, and Linux.


Ask question for RAID
Ask for 0.95 to 1.75 time maxreduce job.
What is advantage of single threaded web application  and database
What is Hive Native communication.


Hive was developed as SQL interface for MapReduce job.
Map Reduce jobs generated by Hive are optimized.
Hive works on structured data and is made to enable data summarization and analysis of large amount of data.
Hive is more proficent than RDBMS queries for big data.
Hive does not store data but works on storage like hadoop.
Hive is installed on a separate machine or master hadoop node.
Hive needs a meta data information which it stores in RDBMS like mysql, etc
On the other hand Hadoop stores its meta data information on master.
Hive RDBMS is always stored on separate system, neither on Hive system or Hadoop Master.

HiveSQL is the Hive Query language and it does not conform to sSQL standard versions
Hiev does not support row level insert,update and deletes as HDFS does not allow them.
Hive does not support transactions
Add extensions to provide better performance in the context of hadoop

The shell is the primary way that we interact with Hive and Hive can be run in non interactive mode with -f option.
Hive can be accessed with CLI, jar, metastore.
Hive SQL is case insensitive.
Hive can also run as server for HIve client applications to connect to it like JDBC applications, Thrift Client etc.

Hive main component is driver. All requests go via driver. 
We can communicate with Hive via a CLI, HIve server, and depricated hive web interface.
Applications can connect to Hive server to submit SQL to Hive Driver.

Driver connect to Metastore database via Metastore Service API.
Driver uses Filesystem Service API to read/write data to HDFS.
Diver uses JobClient API to submit map reduce jobs to Hadoop Cluster.
As soon as user submits SQL query, driver gets meta data from metastore and submit a mapreduce job for reading and gives data to Filesystem to read/write data.

Metasore is central repo of Hive metadata.
Metastore has service API and RDBMS store(default DB is embedded Derby database)
In production, we replace Derby with MySQL as Derby is single threaded. 

Hive does not verify data when it is stored/loaded and validates the data against scheme while reading.
This is called schema on read. 
Since the amount of data is huge and getting stored in distributed cluster, design decision was taken to adopt schema on read. 
Any record not matching the data schema at reading time is dropped.
IN RDBMS, the data is verfied at time of write in table. This is called schema on write.
In Hive initial data reading load is very fast.

SerDe interface allows to instruct Hive as to how a record should be processed.
Serializer is used while writing data to HDFS
Desrializer is used with SQL reading such as SELECT  statements.

HIve generates optimized Map Reduce jobs
Hive has Support of Table-level physical Partiotioning to speed up reading.
RDBMS metastore which is more optimal for random access.
No ACL supported in Hive, so a security drawback.

Partitioning is very important optimzation technique in HIVE
Partition field might not be from data set but an external manual field name to create partition manually.


Refer HIVE 2020 documentation as hive client is changed in latest Hive 3.1.2 version
Default dataset is in hive/examples/files/kv1.txt.
Default separator in dataset is ^A
Study UDF in Hive.

Roller Coaster Analyis. Get some queries in this project using UDF.



How to change Metastore configuration to MySQL database?


FLume is for provide streaming data to hdfs.
TIBCO is another streaming data provider but not as popular as FLUME for HDFS
JDBC channel is very slow and not used.
Memory channel is very fast but volatile so memory data can be lost because of failures like power, crashes etc
FIle channel for lot of data being pushed.
IPC(Inter process Communcation) sink for agene to agent communication: Eg Avro


Flume collector is deployed in staging server which is installed at(just before) firewall in data center.
Flooding of data can bring down the staging server but database/hadoop will not be affected
One cluster of haddop can also be made at staging server which acts as storage cluster
Another hadoop cluster inside the data center which stores active data required for compute.
Relevant data from Storage cluster is send to compute cluster.

Ganglia or customized monitoring system are used for Flume.

Pig is a high level general data flow language
Developed by Yahoo
Hive developed by Facebook
To work in PIG, I need to learn another high level language whereas in Hive I can use SQL.
SO hive is more popular.
Pig can handle unstructured data so I will use Pig when unstructured data is to be processed.
Map Reduce programs will use for enterprise processing as it gives more granular control.
PIG is defunct now. No release on PUG after 2017. 
Install latest version of PIG 0.17 version and also set JAVA_HOME, and set correct path for Hadoop conf path hadoop/etc/hadoop

HBASE is a columnar database with column families 
Based on google Big Table. Hadoop is based on Google File system
Hbase storage is key-value format
Hbase is very performant 
Facebook profile pictures, messaging window is on top of hbase
Casandra was competitor of hbase and developed at Facebook and open sourced.
Casandra was not giving good results for facebook requirements so they moved to Hbase
COlumns in Hbase created on fly 
HBase store data in HDFS
Hive can work on top of HBase.
Hbase store timestamp with each and every value


Flume is popular and sqoop is dying as most of enterprise products are providin thier native ETL tools
Sqoop uses MapReducejob to transfer data between hadooop and external stores like RDBMS
About 10 years back, Sqoop was used to import data from RDBMS into HDFS, transform data in Hadoop Mapreduce, and then export data bakc in RDBMS for reporting
But now hadoop has connectvity with most of the reporting tools
Sqoop can be used to populate tables in Hive or Hbase

Assignment:
1) How to give Hbase table as an input to mapreduce job
2) How ill sqoop export data from hadoop

Hadoop >> Projects >> Spark >> Projects
Practice Scala Course
Projects
Weblog Analysis with Mr/Hive
MovieLens with MR
HR Analysis MR
RollerCoaster Hive (few KPIs with UDF)
Sentiment Analysis MR/Hive most popular project for jobs

Deadline: 30 days, 3 projects
Mail source code, o/p screenshots. readme doc with assumptions/practice followed
support@data-flair.training

Attend Mock Interview
Read DataFlair hadoop interview questions.


WHat is safe mode?
Safe Mode: Read only mode in HDFS
Namenode goes into safe mode when nodes are restarted then some nodes can go in safe mode
when lot of blocks are missing then also node can go into namenode
when does it come out of safe node? dependnds on cluster size, node size, number of blocks and many more configuration setting
you may need to run command manuualy to move ode out of safe mode
its recommneded to let hadoop take time to auto come out of safe mode


How to optimize MR job?
1. Use optimmzed code and datatype like List vs Vector, StringBuffer vs string, best hadoop data types
2. Try to optmize network operations  like sshuffling by using combiner, if applicable. 
   Compress the output of mapper by setting the properti in map-reduce config file. 
   Do not compress at cost of CPU.
3. start and stop of jvm happens for every task which can slow down job. Optimize number of jobs and optimize block size. You can have block size of 256 mb in production
   Data processing time of task shoould be in range of 8-10 times of Jvm start and stop time.
   Reuse JVM by starting batch of JVMs for n number of times. Too many times reusagecan be risky because of some leak or high gc heap.
4. Number of mappers and reducers should be optimized.
5. Enable speculative execution. Some tasks might get stuck and should not define efficency of overall map reduce job.
6. Cluster performance tuning is also important. Node configuration is also important.
7. 

Is the result from mapper or partitioner written to local disk?
Output from mapper is called intermediate output and  contain processed key value pairs. 
Mapper output is in memory buffer.
Map result key-value pairs are comibned by combiner and sent to Paritioner class. 
Parition class sort the pairs and store it on disk
Then this paritioned data is shuffled to nodes running reducers

Where sorting of data is done? On Mapper or Reducer? Which sort algorithm is used?
Sorting is done on both mapper and reducer.
Sort is also during map task using Quick sort Algorithm.
But main sorting is done during shuffle and sort task on reducer node using Merge sort Algorithm.

Q Convince your client for spark based solution for a big data problem

Q Why did you ditch Hadoop?
We didint ditch Hadoop. Talk about architecture and spark'


Q. Plan Hadoop/spark cluster
-100 Gb data daily
-2 PB historical data

Put adjustment factor for increase in data 
-100 Gb X 365 = 36 TB => 50 TB => 100 TB
-2100 TB x RF( say RF =2, +1 for imtermiediate data being written to hdfs)
- 6300 TB
Say 30 (3 X10) TB is available per node
210 nodes for hdfs

Now data went 5 times up in few months
Will need more nodes as data goes up


Q. What is difference between groupBykey/ReduceByKey and which operation among them is the action?
From the two method, which will you recommend?
ReduceByKey and GroupByKey are both transformation function.
reducebykey is having combiner as welll and hence more optimized

Q. Why RDDs are immutable?
Ans Main reason is performance as you are updating in memory cache in a distributed cluster.
Immutability is also needed as you need to maintain lineage across RDDs


Q What are limitations of Apache Spark?


Q How to use Spark over Hadoop?

Action::: show 3 years of Big data experience with big data architect knowledge

Show only 1 big data project in resume
You can add one POC as well.

Which project to add in your profile?
Setupbox project is of the best project to mention.
AAdhar card and IVR enterprise analysis
Convert your existing project to big data project

Add project Details such as:
Requirement Analysis
Architecture
Data Flow
Data Collection


Name of all the components from Data Collection to Visualization along with version.


Most important part is Data Collection
What is your data source?
What is the format of data being collected? xml, csv, json, txt etc
What is character encoding?
How is data being cleansed?
What is the data volume daily and historical? - Neither too low or too high volume to defend it?

Naver say too low number node cluster like 3,5,10 or too high as 1000
Dont say  whole number of nodes say such as 100/200/300/500
Make sure your data size justfies number of nodes
Case can be other projects are using same cluster as yours

Mention Map/reduce/scala

Node Configuration as of today?
Master: Main special dedicated high spec machine
- 256 GB RAM
- 32 core processors
- 20 TB Disk (with RAID)

Slave()
- 64/128 GB RAM
- 16 Core processors
- 10 * 4.7 TB (with JBOD)

Netowrk Speed: 10 GBPS

OS: RedHat(6/7) is preferred over CentOS

Team size: 5 -15

Challenges:
Note your challenges form project executed.
Data :
-Format changes continously
-Different in development than in production
Write a pre processing jobs for data quality/itegrity checks

Integration points :

Data Collection and source can be challenege:



--Drive your own interview--
-Use Buzz words 
-Add buzz words in your profile
-Prepare your question and answers and design your resume around them
-Attach one story/use case with every answer


Keep the numers ready for any answer
Jio architecture requirement was to handle 1 billions eventa were second.















 






























