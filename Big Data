
Error while powering on: This host supports Intel VT-x, but Intel VT-x is disabled.

Intel VT-x might be disabled if it has been disabled in the BIOS/firmware settings or the host has not been power-cycled since changing this setting.

(1) Verify that the BIOS/firmware settings enable Intel VT-x and disable 'trusted execution.'

(2) Power-cycle the host if either of these BIOS/firmware settings have been changed.

(3) Power-cycle the host if you have not done so since installing VMware Player.

(4) Update the host's BIOS/firmware to the latest version.

This host does not support "Intel EPT" hardware assisted MMU virtualization.

Module 'CPUIDEarly' power on failed.

Failed to start the virtual machine.

Step to fix Error:
Restart laptop >>> BIOS Settings >> Enable Intel VT-x / AMD -v


3 case studies 5+ pages

what is a small file problem?
what issues will come with 1 million small files?
what ideal block side should be for HDFS?
how to specify more than one path for storage?

single master in hadoop1 so single point of faiulre. resolved in hadoop2 with multiple instances of master

master stores all the metada in memoery so should have good memory spec
and should be installed on reliable, good spec hardware

Mid size spec with medium ran and high disc (16 tb 2*8 mid size) 

default block size of data is 128 mb in hadoop 3.0. earlier it was 64 Mb.

in hdfs-site.xml on master, we can increase or decrease the replication number

in one datanode also, data is tored on mutiple discs to improve disk i/o performance
Logical volume manager(LVM) for disk decreases i/o performance as it considers all disks as one disk
jbod uses different mount points for each disk and increases the i/o performance

if datanode does not send heartbeats for 2 min(configurable), then namenode considers datanode as dead node.
and once dead node is considered, name node will create replicas of blocks on that node to other nodes
if the node comes up, once Load balancer, kickup, it will delete additional data.
any of the blocks which is not getting accessed is deleted.
ideally what should be replication factor


if 65 data comes for 64 mb block, 64 mb data will go in 1st block, and 2nd block will be just 1 mb to not waster space.
only the last block size can vary

blocks distribution on datanodes is random with few algorithm principles:
1) store data on nearest node from client
2) load on datanode. if slave is too busy, then less blocks will be stored on that machine
3) hadoop is rack aware.

FINISH the quizes on datdalfair nexttel LMS.
HOW to enable trash in hadoop?

Q.I installed hadoop sucessfully. during jps command datanode is not displaying
A: stop hadoop services >> delete hdata dir >> format NN bin/hdfs namenode -format >> start services again

to show all commands
$ hadoop fs

to create dir
$ hadoop fs -mkdir /data001

files cannot be edited in hdfs, but files can be appended with appentToFile command
$ hadoop fs -appendToFile temp /data004/hdfs-site.xml


For small files say size 1 MB, block size will be same as file size. The block size shown on GUI is cluster Block size configured.



Map Reduce:
1) Used when user request for some data from hdfs
2) HDFS data for Request will be divided into mutiple split with each split being served by  individual mapper. 
   Each mapper tries to serve data from local node. 
   In rare cases if one node is too busy the block from that node will move to other node with mapper.
3) HDFS block with key, value pair of byteoffset and value is input to mapper. Map function receives one key-value pair at a time.
4) Output from mapper is called intermediate output and again contain proecssed key value pairs. Mapper output is stored on local disk and not in hdfs.
5) User write the custom program for map
6) Mutiple map function will run on different nodes for each request
7) Intermediate output in form of key, value pair is send to Reducer(s) which can be run one of the nodes running map function.
   The mapper output key pair can be different from input pair
8) The same key with mutiple values from different mappers is fed into one Reducer as Reducer aggregate values of same key from different mappers.
   Key value pairs are sorted by key before reduce funtion kicks.
   Reduce funtion recives one key and complete collection of values for that key.
   Reducer does not have to necessarily follow node locality
9) The reducer output is again key.value pair and called final output is stored in hdfs system.
   Output from each reducer is one single output file.
   The reducer output key pair can be different from input pair.
   FInal output as any hdfs file will be replicated by default.
10) This final output is then send/streamed back to client
11) We can configure this output file to be retained or deleted.

How many mappers can rum simultanously?
Depends on 2 things
1) Limit on number of mappers allowed per node. 
   Right number of parallelism for maps is between 10 to 100 maps/node depending on the RAM size as mappers will run in JVM and CPU cores, 
   for every map you can roughly estimate 1.5 core. For 15 cores you can give 10 mapper.
2) Total size of inputs
Each block will need one map. So number of blocks in input data from hdfs will determine total number of mappers required

How many reducers can run simultaneously?
Number of reducers is way less than number of mappers.
The number of reducers os 0.95 or 1.75 mutiplied by config param mapred.tasktracker.reduce.tasks.maximum.
With 0.95, many redcuers can launch immediately and start reducing mutiple map outputs as maps finish jobs.
With 1.75, faster nodes will finish first round of reducer and launch second wave of reducers to reduce again.

Sometime there can be map only job qithout a reduce with a final output written directly on hdfs and will be replicated.
No need of shuffling and no need of data movement
Map only jobs are more efficient that map reduce jobs as there is no data movement which is a very costly job/
Map reduce application are limited by the bandwidth available on  the cluster
Hadoop allows the user to specify; a combiner function on map output.
Combiner is a mini reducer function which operates only on data generated by one machine saving on data movement.
Comibner acts afiltering or an aggregating step to lessen the number of intermediate keys that are being passed to the reducer.
Mostly the reducer class is also used as combinor for optimization.

Data locality:
Move computation closer to the data.
This minim;izes network congestion
hdfs provides interfaces to move application closer to nodes where data is located.


While executing a mapreduce job on hadoop, for virtual limit exception change in map-site.xml yarn entry to local.

For any class to be "value" for map and reduce class, it has to implement org.apache.hadoop.io.Writeable
Two methods for overwrite
1) write (DataOutput out)
2) readFields (DataInput in)

For any class to be "Key" for map and reduce class, it has to implement org.apache.hadoop.io.WriteableComparable
Three methods for overwrite
1) write (DataOutput out)
2) readFields (DataInput in)
3) compareTo(To)

Process of Map Reduce
1) InputFormat is a class(component )defines how input file is divided and read.
It selects the file from hdfs store and defines the input SPLIT for reading
2) Split is a unit of mapreduce data processing and is a logical representation of a block on hdfs
   Block is physical unit of hdfs.
   Hadoop undersand line boundaries. 
   Lines are not divided between two blocks.
   It terminates the block size to previuos line where size of data is small than block size.
   With xml, it stores as multi line record
   A split will take data from one block but might read partial data from other block(local/remote) to get complete record.
   Therefore Split is logical record boundary and block is physical data boundary
3) RecordReader (RR) is the class that read each split i.e. read data from hdfs.
4) RecordReader keeps giving one key-value pair to Map class till entire split is processed.
   For each split/block their is a mapper.
   map method will be called as many times as key-value pairs are there
   But every mapper class has one setup and cleanup method which will be called only once.
   You can access inputFileName in this setup method. And can do any initialization work in setup method.
   Close or freeup resources in cleanup method.
5) Map result key-value pairs from local disk are comibned by combiner and sent to Paritioner class. 
   Partitioner takes hashvalue of key and all paritioner send same hashvalue(range)/keys data to same reducer node
   The process of moving map output to reducer is known as shuffling
   Shuffling is physcial movement of map results to reducer node.
   As soon as first map task is finished, shuffling starts and data is sent to reducer
   Shuffling is limited by network badnwidth so 10 Gig connection is recommended for intra node connection
6) Sorting is done on key value pairs from paritnioner
7) One Sorted key with mutiple values are given to one Reducer
8) Output of reducer is written to hdfs via outputformat class
   Output format class defines the way how key value pairs will be written to output files.
   Output format decides the location/block where results will be written
9) RecordWriter is the actual class which writes teh reduced task output to persistent hdfs store
   It write one key-value pair at a time.

Note for MapReduce you can customize RecordReader, Mapper (setup, clean, map, combiner), Parition, Reducer(), XMLInoutFormat, XMLOUtputFormat class.

Use jdom in Map Reduce for aml reading as it has simplicity of dom and quick performance of SAX.

FInd best reader for json in MapReduce

org.apache.hadoop.filecache.DistributedCache to get set metadata to distributed cache on alll nodes.
org.apache.hadoop.filecache.DistributedCache.addFiletoClassPath method to add jars to classpath

write hadoop command to execute a jar file for map reduce.

work on MovieAnalysis Project and HR_Analysis Doc.

Impala is a MPP (Massive Parallel Processing) SQL query engine for processing huge volumes of data that is stored in Hadoop cluster. 
It is an open source software which is written in C++ and Java. 
It provides high performance and low latency compared to other SQL engines for Hadoop.

YARN:
ResourceManager daemon on master Node.
NodeManager daenon on slave nodes 
ApplicationMaster is triggered every time a new MapReduce job is triggered
Resource Manager will talk to Node Manager to trigger a application master in a container on slave node.
Resource manager gives containers to application to run. Container is like a memory space and cpu allocatio like 1G, 1 CPU.
Application Master will request RM for one or more containers to run applications which RM will coordinate with NM to allocate more containers.

Kerberos is a computer network security protocol that authenticates service requests between two or more trusted hosts across an untrusted network
Kerberos authentication is currently the default authorization technology used by Microsoft Windows, 
and implementations of Kerberos exist in Apple OS, FreeBSD, UNIX, and Linux.


Ask question for RAID
Ask for 0.95 to 1.75 time maxreduce job.
What is advantage of single threaded web application  and database
What is Hive Native communication.


Hive was developed as SQL interface for MapReduce job.
Map Reduce jobs generated by Hive are optimized.
Hive works on structured data and is made to enable data summarization and analysis of large amount of data.
Hive is more proficent than RDBMS queries for big data.
Hive does not store data but works on storage like hadoop.
Hive is installed on a separate machine or master hadoop node.
Hive needs a meta data information which it stores in RDBMS like mysql, etc
On the other hand Hadoop stores its meta data information on master.
Hive RDBMS is always stored on separate system, neither on Hive system or Hadoop Master.

HiveSQL is the Hive Query language and it does not conform to sSQL standard versions
Hiev does not support row level insert,update and deletes as HDFS does not allow them.
Hive does not support transactions
Add extensions to provide better performance in the context of hadoop

The shell is the primary way that we interact with Hive and Hive can be run in non interactive mode with -f option.
Hive can be accessed with CLI, jar, metastore.
Hive SQL is case insensitive.
Hive can also run as server for HIve client applications to connect to it like JDBC applications, Thrift Client etc.

Hive main component is driver. All requests go via driver. 
We can communicate with Hive via a CLI, HIve server, and depricated hive web interface.
Applications can connect to Hive server to submit SQL to Hive Driver.

Driver connect to Metastore database via Metastore Service API.
Driver uses Filesystem Service API to read/write data to HDFS.
Diver uses JobClient API to submit map reduce jobs to Hadoop Cluster.
As soon as user submits SQL query, driver gets meta data from metastore and submit a mapreduce job for reading and gives data to Filesystem to read/write data.

Metasore is central repo of Hive metadata.
Metastore has service API and RDBMS store(default DB is embedded Derby database)
In production, we replace Derby with MySQL as Derby is single threaded. 

Hive does not verify data when it is stored/loaded and validates the data against scheme while reading.
This is called schema on read. 
Since the amount of data is huge and getting stored in distributed cluster, design decision was taken to adopt schema on read. 
Any record not matching the data schema at reading time is dropped.
IN RDBMS, the data is verfied at time of write in table. This is called schema on write.
In Hive initial data reading load is very fast.

SerDe interface allows to instruct Hive as to how a record should be processed.
Serializer is used while writing data to HDFS
Desrializer is used with SQL reading such as SELECT  statements.

HIve generates optimized Map Reduce jobs
Hive has Support of Table-level physical Partiotioning to speed up reading.
RDBMS metastore which is more optimal for random access.
No ACL supported in Hive, so a security drawback.

Partitioning is very important optimzation technique in HIVE
Partition field might not be from data set but an external manual field name to create partition manually.


Refer HIVE 2020 documentation as hive client is changed in latest Hive 3.1.2 version
Default dataset is in hive/examples/files/kv1.txt.
Default separator in dataset is ^A
Study UDF in Hive.

Roller Coaster Analyis. Get some queries in this project using UDF.



How to change Metastore configuration to MySQL database?


FLume is for provide streaming data to hdfs.
TIBCO is another streaming data provider but not as popular as FLUME for HDFS
JDBC channel is very slow and not used.
Memory channel is very fast but volatile so memory data can be lost because of failures like power, crashes etc
FIle channel for lot of data being pushed.
IPC(Inter process Communcation) sink for agene to agent communication: Eg Avro


Flume collector is deployed in staging server which is installed at(just before) firewall in data center.
Flooding of data can bring down the staging server but database/hadoop will not be affected
One cluster of haddop can also be made at staging server which acts as storage cluster
Another hadoop cluster inside the data center which stores active data required for compute.
Relevant data from Storage cluster is send to compute cluster.

Ganglia or customized monitoring system are used for Flume.

Pig is a high level general data flow language
Developed by Yahoo
Hive developed by Facebook
To work in PIG, I need to learn another high level language whereas in Hive I can use SQL.
SO hive is more popular.
Pig can handle unstructured data so I will use Pig when unstructured data is to be processed.
Map Reduce programs will use for enterprise processing as it gives more granular control.
PIG is defunct now. No release on PUG after 2017. 
Install latest version of PIG 0.17 version and also set JAVA_HOME, and set correct path for Hadoop conf path hadoop/etc/hadoop

Azure IOT hub
AWS Device Twin (or Device Shadow in AWS) is a copy of actual device having meta data of actual device.
Physcial Device/senosr sends the sensed data like Temp to Twin Device.
Rather than using MQTT we use device twin approach for secutiry, using cloud services, reliability of data transfer.
Twin device communicates with Physical device with SAS token for security

Microsoft also give devices which can directly communicate wihtout twin device with Device provisionin service.
Azure also provide firmware which you can put on your chip which will help physical  device to directly communicate wihtout twin device.

IOT 10 Edge is OS for IOT client gateway(like Raspberry Pi as edge Micro Processor) for sensors which will preprocess data from sensors and then send data to cloud.
IOT Gateway can be Raspberry PI with processing or wihtout processing like nodeMCU.

IOT protocol gateway is a fraemwork for IOT communication


on raspberry pi
pip install azure-iot-device
https://docs.microsoft.com/en-us/azure/iot-hub/quickstart-send-telemetry-python

wget code.link

Put device endpoint connection string in code.




























