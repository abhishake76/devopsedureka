
Error while powering on: This host supports Intel VT-x, but Intel VT-x is disabled.

Intel VT-x might be disabled if it has been disabled in the BIOS/firmware settings or the host has not been power-cycled since changing this setting.

(1) Verify that the BIOS/firmware settings enable Intel VT-x and disable 'trusted execution.'

(2) Power-cycle the host if either of these BIOS/firmware settings have been changed.

(3) Power-cycle the host if you have not done so since installing VMware Player.

(4) Update the host's BIOS/firmware to the latest version.

This host does not support "Intel EPT" hardware assisted MMU virtualization.

Module 'CPUIDEarly' power on failed.

Failed to start the virtual machine.

Step to fix Error:
Restart laptop >>> BIOS Settings >> Enable Intel VT-x / AMD -v


3 case studies 5+ pages

what is a small file problem?
what issues will come with 1 million small files?
what ideal block side should be for HDFS?
how to specify more than one path for storage?

single master in hadoop1 so single point of faiulre. resolved in hadoop2 with multiple instances of master

master stores all the metada in memoery so should have good memory spec
and should be installed on reliable, good spec hardware

Mid size spec with medium ran and high disc (16 tb 2*8 mid size) 

default block size of data is 128 mb in hadoop 3.0. earlier it was 64 Mb.

in hdfs-site.xml on master, we can increase or decrease the replication number

in one datanode also, data is tored on mutiple discs to improve disk i/o performance
Logical volume manager(LVM) for disk decreases i/o performance as it considers all disks as one disk
jbod uses different mount points for each disk and increases the i/o performance

if datanode does not send heartbeats for 2 min(configurable), then namenode considers datanode as dead node.
and once dead node is considered, name node will create replicas of blocks on that node to other nodes
if the node comes up, once Load balancer, kickup, it will delete additional data.
any of the blocks which is not getting accessed is deleted.
ideally what should be replication factor


if 65 data comes for 64 mb block, 64 mb data will go in 1st block, and 2nd block will be just 1 mb to not waster space.
only the last block size can vary

blocks distribution on datanodes is random with few algorithm principles:
1) store data on nearest node from client
2) load on datanode. if slave is too busy, then less blocks will be stored on that machine
3) hadoop is rack aware.

FINISH the quizes on datdalfair nexttel LMS.
HOW to enable trash in hadoop?

Q.I installed hadoop sucessfully. during jps command datanode is not displaying
A: stop hadoop services >> delete hdata dir >> format NN bin/hdfs namenode -format >> start services again

to show all commands
$ hadoop fs

to create dir
$ hadoop fs -mkdir /data001

files cannot be edited in hdfs, but files can be appended with appentToFile command
$ hadoop fs -appendToFile temp /data004/hdfs-site.xml


For small files say size 1 MB, block size will be same as file size. The block size shown on GUI is cluster Block size configured.



Map Reduce:
1) Used when user request for some data from hdfs
2) HDFS data for Request will be divided into mutiple split with each split being served by  individual mapper. 
   Each mapper tries to serve data from local node. 
   In rare cases if one node is too busy the block from that node will move to other node with mapper.
3) HDFS block with key, value pair of byteoffset and value is input to mapper. Map function receives one key-value pair at a time.
4) Output from mapper is called intermediate output and again contain proecssed key value pairs. Mapper output is stored on local disk and not in hdfs.
5) User write the custom program for map
6) Mutiple map function will run on different nodes for each request
7) Intermediate output in form of key, value pair is send to Reducer(s) which can be run one of the nodes running map function.
   The mapper output key pair can be different from input pair
8) The same key with mutiple values from different mappers is fed into one Reducer as Reducer aggregate values of same key from different mappers.
   Key value pairs are sorted by key before reduce funtion kicks.
   Reduce funtion recives one key and complete collection of values for that key.
   Reducer does not have to necessarily follow node locality
9) The reducer output is again key.value pair and called final output is stored in hdfs system.
   Output from each reducer is one single output file.
   The reducer output key pair can be different from input pair.
   FInal output as any hdfs file will be replicated by default.
10) This final output is then send/streamed back to client
11) We can configure this output file to be retained or deleted.

How many mappers can rum simultanously?
Depends on 2 things
1) Limit on number of mappers allowed per node. 
   Right number of parallelism for maps is between 10 to 100 maps/node depending on the RAM size as mappers will run in JVM and CPU cores, 
   for every map you can roughly estimate 1.5 core. For 15 cores you can give 10 mapper.
2) Total size of inputs
Each block will need one map. So number of blocks in input data from hdfs will determine total number of mappers required

How many reducers can run simultaneously?
Number of reducers is way less than number of mappers.
The number of reducers os 0.95 or 1.75 mutiplied by config param mapred.tasktracker.reduce.tasks.maximum.
With 0.95, many redcuers can launch immediately and start reducing mutiple map outputs as maps finish jobs.
With 1.75, faster nodes will finish first round of reducer and launch second wave of reducers to reduce again.

Sometime there can be map only job qithout a reduce with a final output written directly on hdfs and will be replicated.
No need of shuffling and no need of data movement
Map only jobs are more efficient that map reduce jobs as there is no data movement which is a very costly job/
Map reduce application are limited by the bandwidth available on  the cluster
Hadoop allows the user to specify; a combiner function on map output.
Combiner is a mini reducer function which operates only on data generated by one machine saving on data movement.
Comibner acts afiltering or an aggregating step to lessen the number of intermediate keys that are being passed to the reducer.
Mostly the reducer class is also used as combinor for optimization.

Data locality:
Move computation closer to the data.
This minim;izes network congestion
hdfs provides interfaces to move application closer to nodes where data is located.


























